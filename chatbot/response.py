import logging
import random
from utils.preprocessing import TextPreprocessor

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

INTRO_PHRASES = [
    "tu le savais-tu ? ‚òùÔ∏èü§ì",
    "oui je le savais-tu ‚òùÔ∏èü§ì",
    "mais enfin...tu le savais-tu ? ‚òùÔ∏èü§ì",
    "tiens-toi bien : je le savais-tu... ‚òùÔ∏èü§ì"
]


def respond_to(prompt: str, model, vectorizer) -> str:
    """
    Prend un prompt utilisateur et renvoie une r√©ponse pr√©dite √† partir d‚Äôun mod√®le et d‚Äôun vectorizer fournis.

    :param prompt: Le texte de l'utilisateur.
    :param model: Le mod√®le ML ou DL entra√Æn√©.
    :param vectorizer: Le vectoriseur utilis√© lors de l'entra√Ænement.
    :return: Une phrase contextuelle.
    """
    logging.info("üì® Nettoyage du prompt utilisateur...")
    preproc = TextPreprocessor()
    prompt_clean = preproc.preprocess(prompt)

    if vectorizer.method == "tfidf":
        X_prompt = vectorizer.transform_tfidf([prompt_clean])
    elif vectorizer.method == "word2vec":
        X_prompt = [prompt_clean.split()]
    else:
        raise ValueError("M√©thode de vectorisation inconnue.")

    prediction = model.predict(X_prompt)[0]

    # Contexte (hack temporaire)
    if hasattr(model, "X_train") and hasattr(model, "y_train"):
        textes = model.X_train
        labels = model.y_train
        contexte = [t for t, l in zip(textes, labels) if l == prediction]
        if contexte:
            sample = random.choice(contexte)
            return f"{random.choice(INTRO_PHRASES)} {sample}"

    return f"{random.choice(INTRO_PHRASES)} Heu‚Ä¶ je n‚Äôai rien trouv√© de coh√©rent √† te dire."
