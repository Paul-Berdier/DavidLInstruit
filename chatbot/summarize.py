# chatbot/summarize_supervised.py
import os
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

import joblib
import numpy as np
import pandas as pd
import nltk
nltk.download("punkt", quiet=True)
from nltk.tokenize import sent_tokenize

from chatbot.preprocessing import clean_text
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import cosine_similarity
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

class Summarizer:
    def __init__(self, data_path="data/summarization_cleaned.csv"):
        self.data_path = data_path
        self.tfidf_path = os.path.join(BASE_DIR, "..", "models", "summarizer", "summary_tfidf_vectorizer.joblib")
        self.ml_model_path = os.path.join(BASE_DIR, "..", "models", "summarizer", "ml_summary_model.joblib")
        self.dl_model_path = "models/summarizer/dl_summary_model.h5"
        self.tokenizer_path = "models/summarizer/tokenizer_summary.joblib"
        self._loaded = False

    def load_or_train(self):
        if self._loaded:
            print("âœ… ModÃ¨les de rÃ©sumÃ© dÃ©jÃ  chargÃ©s.")
            return

        # === Chargement des modÃ¨les existants ===
        if os.path.exists(self.tfidf_path) and os.path.exists(self.ml_model_path) and \
           os.path.exists(self.dl_model_path) and os.path.exists(self.tokenizer_path):
            print("ðŸ“¦ Chargement des modÃ¨les ML & DL de rÃ©sumÃ©...")
            self.tfidf = joblib.load(self.tfidf_path)
            self.clf_ml = joblib.load(self.ml_model_path)
            self.model_dl = load_model(self.dl_model_path)
            self.tokenizer = joblib.load(self.tokenizer_path)
            self.maxlen = self.model_dl.input_shape[1]
            self._loaded = True
            return

        # === PrÃ©paration des donnÃ©es ===
        print("ðŸ“„ Chargement des donnÃ©es de rÃ©sumÃ©...")
        df = pd.read_csv(self.data_path).dropna().sample(frac=1.0, random_state=42)

        # GÃ©nÃ©ration automatique des labels si manquants
        if "label" not in df.columns and "summary" in df.columns:
            print("ðŸ”„ GÃ©nÃ©ration des labels Ã  partir des rÃ©sumÃ©s...")
            texts = df["text"].astype(str).tolist()
            summaries = df["summary"].astype(str).tolist()
            data = []
            for i in range(len(texts)):
                sentences = sent_tokenize(texts[i])
                summary = summaries[i]
                vectorizer = TfidfVectorizer().fit(sentences + [summary])
                vec_sents = vectorizer.transform(sentences)
                vec_summary = vectorizer.transform([summary])
                scores = cosine_similarity(vec_sents, vec_summary).ravel()
                for j, sent in enumerate(sentences):
                    data.append({
                        "text": sent,
                        "label": int(scores[j] >= 0.4)
                    })
            df = pd.DataFrame(data)
            df.to_csv(self.data_path, index=False)
            print(f"âœ… Nouveau dataset supervisÃ© sauvegardÃ© : {self.data_path}")

        texts = df["cleaned"].astype(str).tolist()
        labels = df["label"].astype(int).tolist()
        cleaned = [t for t in texts]

        # === ML : TF-IDF + Logistic Regression ===
        print("ðŸ§  EntraÃ®nement du modÃ¨le ML (TF-IDF + LogisticRegression)...")
        self.tfidf = TfidfVectorizer()
        X_tfidf = self.tfidf.fit_transform(cleaned)
        self.clf_ml = LogisticRegression()
        self.clf_ml.fit(X_tfidf, labels)
        joblib.dump(self.tfidf, self.tfidf_path)
        joblib.dump(self.clf_ml, self.ml_model_path)
        print("âœ… ModÃ¨le ML entraÃ®nÃ© et sauvegardÃ©.")

        # === DL : Embedding + LSTM ===
        print("ðŸ§  EntraÃ®nement du modÃ¨le DL (Embedding + LSTM)...")
        self.tokenizer = Tokenizer()
        self.tokenizer.fit_on_texts(cleaned)
        X_seq = self.tokenizer.texts_to_sequences(cleaned)
        self.maxlen = max(len(seq) for seq in X_seq)
        X_pad = pad_sequences(X_seq, maxlen=self.maxlen)
        y_array = np.array(labels)

        self.model_dl = Sequential()
        self.model_dl.add(Embedding(len(self.tokenizer.word_index) + 1, 64, input_length=self.maxlen))
        self.model_dl.add(LSTM(64))
        self.model_dl.add(Dense(1, activation='sigmoid'))
        self.model_dl.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

        import matplotlib.pyplot as plt

        history = self.model_dl.fit(
            X_pad, y_array,
            epochs=3,
            batch_size=64,
            validation_split=0.2,
            verbose=1
        )

        # ðŸ” Courbes
        plt.plot(history.history['accuracy'], label='Train Acc')
        plt.plot(history.history['val_accuracy'], label='Val Acc')
        plt.title("Ã‰volution de l'accuracy (DL)")
        plt.xlabel("Epoch")
        plt.ylabel("Accuracy")
        plt.legend()
        plt.grid(True)
        plt.savefig("docs/dl_accuracy_curve.png")
        plt.close()

        plt.plot(history.history['loss'], label='Train Loss')
        plt.plot(history.history['val_loss'], label='Val Loss')
        plt.title("Ã‰volution de la loss (DL)")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True)
        plt.savefig("docs/dl_loss_curve.png")
        plt.close()

        self.model_dl.save(self.dl_model_path)
        joblib.dump(self.tokenizer, self.tokenizer_path)
        print("âœ… ModÃ¨le DL entraÃ®nÃ© et sauvegardÃ©.")
        self._loaded = True

    def summarize_ml(self, text):
        self.load_or_train()
        cleaned = clean_text(text)
        X = self.tfidf.transform([cleaned])
        pred = self.clf_ml.predict(X)[0]
        return text if pred == 1 else ""

    def summarize_dl(self, text):
        self.load_or_train()
        cleaned = clean_text(text)
        seq = self.tokenizer.texts_to_sequences([cleaned])
        pad_seq = pad_sequences(seq, maxlen=self.maxlen)
        probs = self.model_dl.predict(pad_seq, verbose=0).ravel()
        return text if probs[0] > 0.5 else ""
